{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spec_cleaning(bo).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UVj4N9eiTQ3v"},"source":["## Google Colab, Drive Configuration & Imports\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLA2hxIefeK3","executionInfo":{"status":"ok","timestamp":1623669556354,"user_tz":-120,"elapsed":27861,"user":{"displayName":"VICTOR AGUADO","photoUrl":"","userId":"11637426900251361343"}},"outputId":"1c13083f-854c-45ce-9cd9-91aab0b4939b"},"source":["from google.colab import drive\n","\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","\n","from PIL import Image\n","import scipy.io as sio\n","import scipy\n","\n","import os\n","import numpy\n","import numpy as np\n","\n","#for loading and visualizing audio files\n","import librosa\n","import librosa.display\n","\n","#to play audio\n","import IPython.display as ipd\n","import skimage.io\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","data_path = '/content/drive/Shareddrives/DeepLearning/Projecte_Final/Data/'\n","spec_path = '/content/drive/Shareddrives/DeepLearning/Projecte_Final/Spectograms/'\n","results_path = '/content/drive/Shareddrives/DeepLearning/Projecte_Final/Results/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oBA4y6j8uiEa"},"source":["def pad_audio(aud_arr, size, axis = 1):\n","    for i in range(size - aud_arr.shape[axis]):\n","        pad = np.ones((aud_arr.shape[0], 1))\n","        aud_arr = np.concatenate((aud_arr, pad), axis)\n","    \n","    return aud_arr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77ahyxosOqq5"},"source":["import pandas as pd\n","\n","def drop_sg_outliers(data_mat,shapes):\n","  \n","  df = pd.DataFrame(shapes)\n","  Q1=df.quantile(0.25)\n","  Q3=df.quantile(0.75)\n","  IQR=Q3-Q1\n","  df_clean=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n","  indices = [index[0] for index in np.argwhere(df_clean.notnull().values).tolist()]\n","  return [data_mat[i] for i in indices]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeX-WWwBM49F"},"source":["## Cleaning Spectograms\n","Check all spectograms, drop the outliers and resize the inliers so all spectograms have the same length"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AjQ41U__OoMy","executionInfo":{"status":"ok","timestamp":1623521848457,"user_tz":-120,"elapsed":4396854,"user":{"displayName":"IU FERRER","photoUrl":"","userId":"02943294489117180239"}},"outputId":"8903ce24-7e94-4e3a-8b10-6e1d30755a2a"},"source":["words = ['catapulta', 'hola', 'iu', 'mar', 'taula', 'victor']\n","dts = ['Train', 'Test']\n","\n","originals_path = spec_path + \"original/\"\n","resized_path = spec_path + \"resized/\"\n","\n","word_max = []\n","word_index = {}\n","\n","for word in words:\n","    print(\"------\", word)\n","    durations = []\n","    for dt in dts:\n","        \n","        directory = originals_path + word + \"/\" + dt + \"/\"\n","        n_spec = len(os.listdir(directory))\n","\n","        for i in range(n_spec):\n","            file_name = directory + f'/%d.png'%(i+1)\n","            try:\n","                image = img.imread(file_name)  # Convert images to matrix\n","                durations.append(image.shape[1])\n","            except:\n","                durations.append(-1)  # Some images are corrupted\n","\n","            if (i % 50 == 0):\n","                print(\"Vamo por la : \", i, \"/\", n_spec)\n","    \n","    # Drop spectograms following the IQR\n","\n","    df = pd.DataFrame(durations)\n","    Q1=df.quantile(0.25)\n","    Q3=df.quantile(0.75)\n","    IQR=Q3-Q1\n","    df_clean=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n","    indices = [index[0] for index in np.argwhere(df_clean.notnull().values).tolist()]\n","\n","    word_index[word] = indices\n","    word_max.append( int(df_clean.max()) )\n","\n","print(\"word maxes:\", word_max)\n","\n","for word in words:              # Resize all spectograms to the largest one\n","    print(\"------\", word)\n","    for dt in dts:\n","        for index in word_index[word]:\n","            index += 1\n","            try:\n","                image = Image.open(originals_path + word + \"/\" + dt + f'/%d.png'%(index))\n","                image = image.resize(( max(word_max), 128 ), Image.ANTIALIAS)\n","                image.save(resized_path + word + \"/\" + dt + f'/%d.png'%(index))\n","            except:\n","                continue\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------ catapulta\n","Vamo por la :  0 / 447\n","Vamo por la :  50 / 447\n","Vamo por la :  100 / 447\n","Vamo por la :  150 / 447\n","Vamo por la :  200 / 447\n","Vamo por la :  250 / 447\n","Vamo por la :  300 / 447\n","Vamo por la :  350 / 447\n","Vamo por la :  400 / 447\n","Vamo por la :  0 / 125\n","Vamo por la :  50 / 125\n","Vamo por la :  100 / 125\n","------ hola\n","Vamo por la :  0 / 1000\n","Vamo por la :  50 / 1000\n","Vamo por la :  100 / 1000\n","Vamo por la :  150 / 1000\n","Vamo por la :  200 / 1000\n","Vamo por la :  250 / 1000\n","Vamo por la :  300 / 1000\n","Vamo por la :  350 / 1000\n","Vamo por la :  400 / 1000\n","Vamo por la :  450 / 1000\n","Vamo por la :  500 / 1000\n","Vamo por la :  550 / 1000\n","Vamo por la :  600 / 1000\n","Vamo por la :  650 / 1000\n","Vamo por la :  700 / 1000\n","Vamo por la :  750 / 1000\n","Vamo por la :  800 / 1000\n","Vamo por la :  850 / 1000\n","Vamo por la :  900 / 1000\n","Vamo por la :  950 / 1000\n","Vamo por la :  0 / 184\n","Vamo por la :  50 / 184\n","Vamo por la :  100 / 184\n","Vamo por la :  150 / 184\n","------ iu\n","Vamo por la :  0 / 676\n","Vamo por la :  50 / 676\n","Vamo por la :  100 / 676\n","Vamo por la :  150 / 676\n","Vamo por la :  200 / 676\n","Vamo por la :  250 / 676\n","Vamo por la :  300 / 676\n","Vamo por la :  350 / 676\n","Vamo por la :  400 / 676\n","Vamo por la :  450 / 676\n","Vamo por la :  500 / 676\n","Vamo por la :  550 / 676\n","Vamo por la :  600 / 676\n","Vamo por la :  650 / 676\n","Vamo por la :  0 / 196\n","Vamo por la :  50 / 196\n","Vamo por la :  100 / 196\n","Vamo por la :  150 / 196\n","------ mar\n","Vamo por la :  0 / 776\n","Vamo por la :  50 / 776\n","Vamo por la :  100 / 776\n","Vamo por la :  150 / 776\n","Vamo por la :  200 / 776\n","Vamo por la :  250 / 776\n","Vamo por la :  300 / 776\n","Vamo por la :  350 / 776\n","Vamo por la :  400 / 776\n","Vamo por la :  450 / 776\n","Vamo por la :  500 / 776\n","Vamo por la :  550 / 776\n","Vamo por la :  600 / 776\n","Vamo por la :  650 / 776\n","Vamo por la :  700 / 776\n","Vamo por la :  750 / 776\n","Vamo por la :  0 / 177\n","Vamo por la :  50 / 177\n","Vamo por la :  100 / 177\n","Vamo por la :  150 / 177\n","------ taula\n","Vamo por la :  0 / 703\n","Vamo por la :  50 / 703\n","Vamo por la :  100 / 703\n","Vamo por la :  150 / 703\n","Vamo por la :  200 / 703\n","Vamo por la :  250 / 703\n","Vamo por la :  300 / 703\n","Vamo por la :  350 / 703\n","Vamo por la :  400 / 703\n","Vamo por la :  450 / 703\n","Vamo por la :  500 / 703\n","Vamo por la :  550 / 703\n","Vamo por la :  600 / 703\n","Vamo por la :  650 / 703\n","Vamo por la :  700 / 703\n","Vamo por la :  0 / 178\n","Vamo por la :  50 / 178\n","Vamo por la :  100 / 178\n","Vamo por la :  150 / 178\n","------ victor\n","Vamo por la :  0 / 770\n","Vamo por la :  50 / 770\n","Vamo por la :  100 / 770\n","Vamo por la :  150 / 770\n","Vamo por la :  200 / 770\n","Vamo por la :  250 / 770\n","Vamo por la :  300 / 770\n","Vamo por la :  350 / 770\n","Vamo por la :  400 / 770\n","Vamo por la :  450 / 770\n","Vamo por la :  500 / 770\n","Vamo por la :  550 / 770\n","Vamo por la :  600 / 770\n","Vamo por la :  650 / 770\n","Vamo por la :  700 / 770\n","Vamo por la :  750 / 770\n","Vamo por la :  0 / 139\n","Vamo por la :  50 / 139\n","Vamo por la :  100 / 139\n","word maxes: [98, 68, 54, 52, 65, 74]\n","------ catapulta\n","------ hola\n","------ iu\n","------ mar\n","------ taula\n","------ victor\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YjVngyhHNpBW"},"source":["## Creating matrices\n","Read all spectograms and create Test and Train matrices for each word"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuR01ZP_dGaf","executionInfo":{"status":"ok","timestamp":1623522418644,"user_tz":-120,"elapsed":12051,"user":{"displayName":"IU FERRER","photoUrl":"","userId":"02943294489117180239"}},"outputId":"5dc79e89-03ef-4731-cf7d-a5271d605e9c"},"source":["words = ['catapulta', 'hola', 'iu', 'mar', 'taula', 'victor']\n","dts = ['Train', 'Test']\n","word2num = {'catapulta': 0, \n","            'hola': 1, \n","            'iu': 2, \n","            'mar': 3, \n","            'taula': 4, \n","            'victor': 5}\n","\n","leng = max(word_max) \n","\n","for dt in dts:\n","    for word in words:\n","        print(\"-----\",word)\n","\n","        directory = spec_path + \"resized/\" + word + \"/\" + dt + \"/\"\n","        img_list = os.listdir(directory)\n","        data_mat = np.zeros( (128, leng, len(img_list)) )\n","        i = 0\n","        for img_name in img_list:\n","            image = img.imread(directory + img_name)\n","            try:   # As the image might be in the other dataset (Train / Test)\n","                data_mat[:,:,i] = image\n","            except:\n","                i += 1\n","                continue\n","            i += 1\n","        labels_mat = np.array( [word2num[word]] * len(img_list) )\n","        labels_mat = np.reshape( labels_mat, (len(img_list), 1) )\n","\n","        word_dict = {'X': data_mat, 'y': labels_mat}\n","        scipy.io.savemat(data_path + 'SG_mat/' + word + \"/\" + dt + \"/\" + \"%s_%s.mat\"%(word,dt), word_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----- catapulta\n","----- hola\n","----- iu\n","----- mar\n","----- taula\n","----- victor\n","----- catapulta\n","----- hola\n","----- iu\n","----- mar\n","----- taula\n","----- victor\n"],"name":"stdout"}]}]}